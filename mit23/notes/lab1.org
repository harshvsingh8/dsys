
** Task

  - Implement a worker process that calls application Map and Reduce
    functions.
  - Handles reading and writing files,
  - A coordinator process that hands out tasks to workers and copes
    with failed workers.


** Design Ideas
   
The MR applications adhere to the following contract:

#+begin_src go

// document - name of the file being processed.
// value    - the input file content.
// @return  - list of intermediate key-value pairs. 
func Map(document string, value string) (res []mr.KeyValue) { }

// key      - input key value (in this reducer scope)
// values   - linked values for the given key from across the mappers.
// @return  - one string result for the given key value.
func Reduce(key string, values []string) string { }

// intermediate key-value struct.
type KeyValue struct {
	Key   string
	Value string
}

#+end_src

The ~mrcoordinator~ makes (instantiates and starts server) a
coordinator, and then waits for it to finish (i.e. until ~Done()~
returns true). The reducer task count is fixed to 10, and the input
file count determines the mapper tasks count. 

#+begin_src go

	m := mr.MakeCoordinator(os.Args[1:], 10)
	for m.Done() == false {
		time.Sleep(time.Second)
	}

#+end_src

The tasks will be of two kinds, Mapper and Reducer, and their
cardinality will be as:

| Mapper tasks       | M, input files count                |
| Reducer tasks      | R, 10 - hard-coded                  |


Several files are involved during the execution of an MR
application.

| Type               | Cardinality                         | Naming                  |
|--------------------+-------------------------------------+-------------------------|
| Input files        | Given to ~MakeCoordinator~ as files | As given by the caller. |
| Intermediate files | M*R                                 | mr-int-{r}-{m}.json     |
| Output files       | R                                   | mr-out-{r}.json         |


The implementation can happen in 3 steps - each with its own
verifiable results:

 1. Request/Response structure - Worker Task - Task Table -
    NO-OP/Sleep action - Done().
 2. Mapper invocation - intermediate files generation.
 3. Reduce invocation - output files generation,

MR application will track each task with the following states:

| Status      | Note                                                                       |
|-------------+----------------------------------------------------------------------------|
| New         | New task, not scheduled yet.                                               |
| Scheduled   | It was send for execution to a worker.                                     |
| Delayed     | Too much time has elapsed since last scheduled, can be re-scheduled        |
| Rescheduled | The task has been re-scheduled - as the previous attempt was unsuccessful. |
| Completed   | The task is done.                                                          |

And, a task can be one of the following kinds:

| Task   | Note                                                                            |
|--------+---------------------------------------------------------------------------------|
| Map    | Mapper task, takes one input file and generated R intermediate k-v data files.  |
| Reduce | Reducer task, takes M intermediate k-v data files and generate one output file. |
| Wait   | There is no pending task at this moment but hold on, recheck after some time.   |
| None   | All tasks are completed, there are none to do - can exit the worker.            |

The workers will ask for the next task from the master, which will
provide one of the types listed above, it then goes on to execute that
and then comes back again to ack its completion and for the next task
to pick.

A worker will request for its next task with the following request details:

*RPC* : ~GetNextTask(workerId: string): WorkerTask~

| Field    | Notes                                                                                     |
|----------+-------------------------------------------------------------------------------------------|
| WorkerId | unique worker id - to identify itself to the master - used for tracking/logging purposes. |

The master will respond with the following details:

| Field        | Notes                                                               |
|--------------+---------------------------------------------------------------------|
| TaskId       | Unique identifier for this task.                                    |
| TaskType     | On of these: Map/Reduce/Wait/None                                   |
| TaskSlot     | Which Map/Reduce task slot to pick for execution.                   |
| TaskFileName | Input file for a Map task, or Output file for a Reduce task.        |
| MapperCount  | Total number of mappers - depends on input file count.              |
| ReducerCount | Total number of reducers - hard-coded to 10, but supplied as param. |

After completion of a Map or Reduce task, the worker will ack the
master with the following RPC/

*RPC*: ~AckTaskCompletion(taskId: string)~

The master would keep a task table for book-keeping all pending and
current executing tasks. The following table indicates a
representative option:

| TaskId | TaskType | TaskSlot | ScheduleTo | ScheduledAt | Status    |
|--------+----------+----------+------------+-------------+-----------|
| M_00   | Map      |        0 | <w-p>      | <date-time> | Scheduled |
| M_01   | Map      |        1 | <w-q>      | <date-time> | Scheduled |
| ...    | Map      |          |            |             | New       |
| R_00   | Reduce   |        0 |            |             | New       |
|        |          |          |            |             |           |

The *master* will observe the following logic for the task scheduling:

 1. If there is any *new* Map task, the it supply that for execution.
 2. If there is any scheduled Map task which is long *delayed* for
    completion, it reschedules that.
 3. If all map tasks are completed, then it schedules Reduce tasks -
    *new* or *delayed* one.
 4. If all task are marked completed then respond with *None*.
 5. If there is no task to schedule at this moment as they are either
    completed or scheduled (but not delayed), then it respond with
    *Wait* task.

A *worker* in turn would execute the following strategy (in loop):

 1. Requests the master for the next task.
 2. If next task is *None* then exit the worker.
 3. If next task is *Wait* then sleep for the configured seconds, and
    recheck for the next task.
 4. If next task is of Map type:
    1. Take input file from the task request.
    2. Read its whole content.
    3. Supply file name and its content to the *Mapper function*.
    4. Gets ~KeyValue~ list result from the mapper:
       1. Group keys on Reducer slots (hash on key value in R slots)
       2. Save values as JSON temp files for each reducer slots.
       3. Rename JSON temp file as per the expected intermediate file
          name (as atomic file rename operation).
       4. Call RPC *AckTaskCompletion* to the master.
 5. If the next is of *Reduce* type:
     1. Assemble all intermediate files for the given reduce slot.
     2. Club all Values across all files for all Key.
     3. For each Key:
	1. Call *Reducer function*
	2. Set aside the reduced value (the returned value).
     4. Create a tmp result file, and for each key and its reduced
        value - add a result line in the expected format.
     5. Atomically rename the output file.
     6. Call RPC *AckTaskCompletion* to the master.
      

** Solution



** Sample Runs and Tests
